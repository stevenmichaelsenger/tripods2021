{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b60d2ab",
   "metadata": {},
   "source": [
    "# Using Noise by Perturbing Each Image in a Certain Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a05f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import random\n",
    "\n",
    "class RegionalNoise():\n",
    "    # returns the CIFAR-10 data set (info here: https://keras.io/api/datasets/cifar10/)\n",
    "    # The data set loaded is has 50,000 training images and 10,000 testing images, but the function below only takes\n",
    "    # a certain number train_size of training images from the original training set. The rest of the training examples\n",
    "    # are added to the test images.\n",
    "    def generate_data(train_size):\n",
    "        # loads the data in 50000,10000 form with labels (y) as integers, not one-hot\n",
    "        (x_train_bef, y_train_bef), (x_test_bef, y_test_bef) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "        # shuffles 50000 training data\n",
    "        np.random.shuffle(x_train_bef)\n",
    "        np.random.shuffle(y_train_bef)\n",
    "\n",
    "        # cuts off train_size amounts of sample data\n",
    "        x_train = x_train_bef[:train_size]\n",
    "        y_train = y_train_bef[:train_size]\n",
    "\n",
    "        # adds the rest of the training data to the test data\n",
    "        x_test = np.concatenate((x_train_bef[train_size:], x_test_bef), axis=0)\n",
    "        y_test = np.concatenate((y_train_bef[train_size:], y_test_bef), axis=0)\n",
    "\n",
    "        # turns image arrays into floats just so that everything is a float, not an int\n",
    "        x_train = x_train.astype(np.float)\n",
    "        x_test = x_test.astype(np.float)\n",
    "\n",
    "        # turns label data into one-hot form\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    # looks at the training data, clones it, but then creates small changes to each image\n",
    "    # Currently, it takes 3 random 5x5 sections of each image and perturbs each section.\n",
    "    # the input it original training data which is not changed, but copied and then perturbed: the perturbed result\n",
    "    # is returned\n",
    "    def create_perturbed_clone(x_train, y_train):\n",
    "        # new objects that are copied: changing x_train_pert won't change x_train\n",
    "        x_train_pert = x_train.copy()\n",
    "        y_train_pert = y_train.copy()\n",
    "\n",
    "        # for each image in x_train_pert we look at a random location and perturb the 5x5 region\n",
    "        length=x_train_pert.shape[0]\n",
    "        for i in range(length):\n",
    "            # perturbation is done 3 times\n",
    "            for repeat in range(3):\n",
    "                loc_x = random.randrange(2,30) # x-coord of center of 5x5 region\n",
    "                loc_y = random.randrange(2,30) # y-coord of center of 5x5 region\n",
    "\n",
    "                # each pixel and each RGB value is perturbed\n",
    "                for u in [-2,-1,0,1,2]:\n",
    "                    for v in [-2,-1,0,1,2]:\n",
    "                        x_train_pert[i][loc_x+u][loc_y+v][0] += 50*(random.random()-0.5)\n",
    "                        x_train_pert[i][loc_x+u][loc_y+v][1] += 50*(random.random()-0.5)\n",
    "                        x_train_pert[i][loc_x+u][loc_y+v][2] += 50*(random.random()-0.5)\n",
    "\n",
    "        return x_train_pert, y_train_pert\n",
    "\n",
    "    # takes training data and adds a perturbed copy to make the training set enlarge_factor times larger\n",
    "    def create_noise(x_train, y_train, enlarge_factor):\n",
    "        x_train_noisy = x_train.copy()\n",
    "        y_train_noisy = y_train.copy()\n",
    "        for i in range(enlarge_factor-1):\n",
    "            x_add, y_add = RegionalNoise.create_perturbed_clone(x_train, y_train)\n",
    "            x_train_noisy = np.concatenate((x_train_noisy, x_add))\n",
    "            y_train_noisy = np.concatenate((y_train_noisy, y_add))\n",
    "        return x_train_noisy, y_train_noisy\n",
    "\n",
    "    # creates network: we use a convolutional neural network which makes sense for this problem\n",
    "    def create_model():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax')) # output lyer is 10-dimension one-hot, so softmax is used\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def run_model(Epochs, BatchSize, trainSize, Set_enlarge, noisy=False):\n",
    "        x_train, y_train, x_test, y_test = RegionalNoise.generate_data(trainSize)\n",
    "        if noisy:\n",
    "            x_train, y_train = RegionalNoise.create_noise(x_train, y_train, Set_enlarge)\n",
    "        MODEL = RegionalNoise.create_model()\n",
    "        MODEL.fit(x=x_train, y=y_train, epochs=Epochs, batch_size=BatchSize)\n",
    "        MODEL.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebe99bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bfe4a64f607d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Whether with or without noise, the model is accurate about 0.1 of the time on the test data, i.e. it is no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# better than a random guesser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mRegionalNoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet_enlarge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-34c861af48e7>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(Epochs, BatchSize, trainSize, Set_enlarge, noisy)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegionalNoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet_enlarge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mMODEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mMODEL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_noise' is not defined"
     ]
    }
   ],
   "source": [
    "# Whether with or without noise, the model is accurate about 0.1 of the time on the test data, i.e. it is no\n",
    "# better than a random guesser.\n",
    "RegionalNoise.run_model(8,64,6000, noisy=True, Set_enlarge=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfca5a",
   "metadata": {},
   "source": [
    "# Using Noise by Perturbing Each Pixel with Equal Probability (Salt and Pepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9675400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import random\n",
    "\n",
    "class Salt_and_Pepper():\n",
    "    # returns the CIFAR-10 data set (info here: https://keras.io/api/datasets/cifar10/)\n",
    "    # The data set loaded is has 50,000 training images and 10,000 testing images, but the function below only takes\n",
    "    # a certain number train_size of training images from the original training set. The rest of the training examples\n",
    "    # are added to the test images.\n",
    "    def generate_data(train_size):\n",
    "        # loads the data in 50000,10000 form with labels (y) as integers, not one-hot\n",
    "        (x_train_bef, y_train_bef), (x_test_bef, y_test_bef) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "        # shuffles 50000 training data\n",
    "        np.random.shuffle(x_train_bef)\n",
    "        np.random.shuffle(y_train_bef)\n",
    "\n",
    "        # cuts off train_size amounts of sample data\n",
    "        x_train = x_train_bef[:train_size]\n",
    "        y_train = y_train_bef[:train_size]\n",
    "\n",
    "        # adds the rest of the training data to the test data\n",
    "        x_test = np.concatenate((x_train_bef[train_size:], x_test_bef), axis=0)\n",
    "        y_test = np.concatenate((y_train_bef[train_size:], y_test_bef), axis=0)\n",
    "\n",
    "        # turns image arrays into floats just so that everything is a float, not an int\n",
    "        x_train = x_train.astype(np.float)\n",
    "        x_test = x_test.astype(np.float)\n",
    "\n",
    "        # turns label data into one-hot form\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    # looks at the training data, clones it, but then creates small changes to each image\n",
    "    # Currently, it perturbs a pixel with probability 0.1 (and changes each RGB value by a random amount)\n",
    "    # the input is original training data which is not changed, but copied and then perturbed: the perturbed result\n",
    "    # is returned\n",
    "    def create_perturbed_clone(x_train, y_train):\n",
    "        # new objects that are copied: changing x_train_pert won't change x_train\n",
    "        x_train_pert = x_train.copy()\n",
    "        y_train_pert = y_train.copy()\n",
    "\n",
    "        # for each image in x_train_pert we perturb each pixel with probability 0.1\n",
    "        shape=x_train_pert.shape\n",
    "        for i in range(shape[0]):\n",
    "            for x in range(shape[1]):\n",
    "                for y in range(shape[2]):\n",
    "                    num = random.random()\n",
    "                    if num<0.1:\n",
    "                        x_train_pert[i][x][y][0] += 100*(random.random()-0.5)\n",
    "                        x_train_pert[i][x][y][1] += 100*(random.random()-0.5)\n",
    "                        x_train_pert[i][x][y][2] += 100*(random.random()-0.5)\n",
    "\n",
    "        return x_train_pert, y_train_pert\n",
    "\n",
    "    # takes training data and adds a perturbed copy to make the training set enlarge_factor times larger\n",
    "    def create_noise(x_train, y_train, enlarge_factor):\n",
    "        x_train_noisy = x_train.copy()\n",
    "        y_train_noisy = y_train.copy()\n",
    "        for i in range(enlarge_factor-1):\n",
    "            x_add, y_add = Salt_and_Pepper.create_perturbed_clone(x_train, y_train)\n",
    "            x_train_noisy = np.concatenate((x_train_noisy, x_add))\n",
    "            y_train_noisy = np.concatenate((y_train_noisy, y_add))\n",
    "        return x_train_noisy, y_train_noisy\n",
    "\n",
    "    # creates network: we use a convolutional neural network which makes sense for this problem\n",
    "    def create_model():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax')) # output lyer is 10-dimension one-hot, so softmax is used\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def run_model(Epochs, BatchSize, trainSize, Set_enlarge, noisy=False):\n",
    "        x_train, y_train, x_test, y_test = Salt_and_Pepper.generate_data(trainSize)\n",
    "        if noisy:\n",
    "            x_train, y_train = Salt_and_Pepper.create_noise(x_train, y_train, Set_enlarge)\n",
    "        MODEL = Salt_and_Pepper.create_model()\n",
    "        MODEL.fit(x=x_train, y=y_train, epochs=Epochs, batch_size=BatchSize)\n",
    "        MODEL.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34deceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "938/938 [==============================] - 96s 99ms/step - loss: 4.3992 - accuracy: 0.1427\n",
      "Epoch 2/4\n",
      "938/938 [==============================] - 95s 101ms/step - loss: 2.1106 - accuracy: 0.5584\n",
      "Epoch 3/4\n",
      "938/938 [==============================] - 100s 107ms/step - loss: 1.5498 - accuracy: 0.8324\n",
      "Epoch 4/4\n",
      "938/938 [==============================] - 123s 132ms/step - loss: 1.3866 - accuracy: 0.8853\n",
      "1688/1688 [==============================] - 29s 17ms/step - loss: 7.8101 - accuracy: 0.0998\n"
     ]
    }
   ],
   "source": [
    "# Again, not much effect\n",
    "Salt_and_Pepper.run_model(4,64,6000, noisy=True, Set_enlarge=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f810223",
   "metadata": {},
   "source": [
    "# Adding Gaussian Noise to Each Pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368d4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import random\n",
    "\n",
    "class GaussianEvery():\n",
    "    # returns the CIFAR-10 data set (info here: https://keras.io/api/datasets/cifar10/)\n",
    "    # The data set loaded is has 50,000 training images and 10,000 testing images, but the function below only takes\n",
    "    # a certain number train_size of training images from the original training set. The rest of the training examples\n",
    "    # are added to the test images.\n",
    "    def generate_data(train_size):\n",
    "        # loads the data in 50000,10000 form with labels (y) as integers, not one-hot\n",
    "        (x_train_bef, y_train_bef), (x_test_bef, y_test_bef) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "        # shuffles 50000 training data\n",
    "        np.random.shuffle(x_train_bef)\n",
    "        np.random.shuffle(y_train_bef)\n",
    "\n",
    "        # cuts off train_size amounts of sample data\n",
    "        x_train = x_train_bef[:train_size]\n",
    "        y_train = y_train_bef[:train_size]\n",
    "\n",
    "        # adds the rest of the training data to the test data\n",
    "        x_test = np.concatenate((x_train_bef[train_size:], x_test_bef), axis=0)\n",
    "        y_test = np.concatenate((y_train_bef[train_size:], y_test_bef), axis=0)\n",
    "\n",
    "        # turns image arrays into floats just so that everything is a float, not an int\n",
    "        x_train = x_train.astype(np.float)\n",
    "        x_test = x_test.astype(np.float)\n",
    "\n",
    "        # turns label data into one-hot form\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    # looks at the training data, clones it, but then creates small changes to each image\n",
    "    # Currently, it adds a random amount (that varies according to Gaussian distribution) to each RGB value of each\n",
    "    # pixel\n",
    "    # the input is original training data which is not changed, but copied and then perturbed: the perturbed result\n",
    "    # is returned\n",
    "    def create_perturbed_clone(x_train, y_train):\n",
    "        # new objects that are copied: changing x_train_pert won't change x_train\n",
    "        x_train_pert = x_train.copy()\n",
    "        y_train_pert = y_train.copy()\n",
    "\n",
    "        # for each image in x_train_pert and each pixel of the image we perturb the RGB values by a Gaussian amount\n",
    "        shape=x_train_pert.shape\n",
    "        for i in range(shape[0]):\n",
    "            for x in range(shape[1]):\n",
    "                for y in range(shape[2]):\n",
    "                    x_train_pert[i][x][y][0] += random.gauss(0,20)\n",
    "                    x_train_pert[i][x][y][1] += random.gauss(0,20)\n",
    "                    x_train_pert[i][x][y][2] += random.gauss(0,20)\n",
    "\n",
    "        return x_train_pert, y_train_pert\n",
    "\n",
    "    # takes training data and adds a perturbed copy to make the training set enlarge_factor times larger\n",
    "    def create_noise(x_train, y_train, enlarge_factor):\n",
    "        x_train_noisy = x_train.copy()\n",
    "        y_train_noisy = y_train.copy()\n",
    "        for i in range(enlarge_factor-1):\n",
    "            x_add, y_add = GaussianEvery.create_perturbed_clone(x_train, y_train)\n",
    "            x_train_noisy = np.concatenate((x_train_noisy, x_add))\n",
    "            y_train_noisy = np.concatenate((y_train_noisy, y_add))\n",
    "        return x_train_noisy, y_train_noisy\n",
    "\n",
    "    # creates network: we use a convolutional neural network which makes sense for this problem\n",
    "    def create_model():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax')) # output lyer is 10-dimension one-hot, so softmax is used\n",
    "\n",
    "        \n",
    "        #opt = keras.optimizers.SGD(learning_rate = 0.1, momentum = 0.9)\n",
    "        #model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def run_model(Epochs, BatchSize, trainSize, Set_enlarge, noisy=False):\n",
    "        x_train, y_train, x_test, y_test = GaussianEvery.generate_data(trainSize)\n",
    "        if noisy:\n",
    "            x_train, y_train = GaussianEvery.create_noise(x_train, y_train, Set_enlarge)\n",
    "        MODEL = GaussianEvery.create_model()\n",
    "        MODEL.fit(x=x_train, y=y_train, epochs=Epochs, batch_size=BatchSize)\n",
    "        MODEL.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8168779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 111s 116ms/step - loss: 5.2256 - accuracy: 0.1770\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 116s 124ms/step - loss: 1.9546 - accuracy: 0.6954\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 116s 124ms/step - loss: 1.4934 - accuracy: 0.8723\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 116s 124ms/step - loss: 1.3616 - accuracy: 0.9004\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 117s 125ms/step - loss: 1.3103 - accuracy: 0.9167\n",
      "1688/1688 [==============================] - 33s 19ms/step - loss: 7.8034 - accuracy: 0.1002\n"
     ]
    }
   ],
   "source": [
    "GaussianEvery.run_model(5, 64, 6000, Set_enlarge=10, noisy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a48c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011e5897",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 703. MiB for an array with shape (30000, 32, 32, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8d078f8cea9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGaussianEvery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet_enlarge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-646ed5b530fa>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(Epochs, BatchSize, trainSize, Set_enlarge, noisy)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianEvery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianEvery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet_enlarge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mMODEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianEvery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mMODEL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-646ed5b530fa>\u001b[0m in \u001b[0;36mcreate_noise\u001b[1;34m(x_train, y_train, enlarge_factor)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menlarge_factor\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mx_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianEvery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_perturbed_clone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mx_train_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_noisy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0my_train_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_noisy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_train_noisy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_noisy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 703. MiB for an array with shape (30000, 32, 32, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "GaussianEvery.run_model(Epochs=2, BatchSize=128, trainSize=15000, Set_enlarge=10, noisy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b42fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1172/1172 [==============================] - 717s 609ms/step - loss: 5.5415 - accuracy: 0.1246\n",
      "Epoch 2/2\n",
      "1172/1172 [==============================] - 729s 622ms/step - loss: 2.6190 - accuracy: 0.3976\n",
      "1407/1407 [==============================] - 70s 49ms/step - loss: 4.8277 - accuracy: 0.0986\n"
     ]
    }
   ],
   "source": [
    "Salt_and_Pepper.run_model(Epochs=2, BatchSize=128, trainSize=15000, Set_enlarge=10, noisy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74928410",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36db27",
   "metadata": {},
   "source": [
    "The neural net is unable to generalize off of 6,000 training samples (54,000 testing) in general. This is also the case when we add the three types of noise from above.\n",
    "\n",
    "We now investigate if we increase trainin size and see what noise does. We change the architecture of the network a little to account for this (still using relu activation and a cnn, however).\n",
    "\n",
    "Architecture:\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        #model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax')) # output lyer is 10-dimension one-hot, so softmax is used\n",
    "\n",
    "\n",
    "15,000 train, 45,000 test: 4 epochs, batch size 128, noise multiplies training set by 10\n",
    "    \n",
    "Without Noise: (12 epochs used)\n",
    "\n",
    "Trial |validation loss |validation accuracy |training loss |training accuracy\n",
    "-|-|-|-|-\n",
    "1|3.1619|0.1016|2.4222|0.3392\n",
    "2|2.8962|0.0966|2.4348|0.2762\n",
    "3|3.4867|0.0999|2.3478|0.3993\n",
    "4|2.8908|0.1022|2.4825|0.2516\n",
    "5|2.9787|0.0966|2.3811|0.3023\n",
    "\n",
    "Without Noise: (40 epochs used)\n",
    "\n",
    "Trial |validation loss |validation accuracy |training loss |training accuracy\n",
    "-|-|-|-|-\n",
    "1|7.1343|0.0982|1.7263|0.8225\n",
    "\n",
    "With Regional Noise: (2 epochs used)\n",
    "\n",
    "Trial |validation loss |validation accuracy |training loss |training accuracy\n",
    "-|-|-|-|-\n",
    "1|5.0367|0.0994|2.5729|0.4287\n",
    "2|4.9546|0.0970|2.4516|0.4538\n",
    "\n",
    "With Salt and Pepper Noise: (2 epochs used)\n",
    "\n",
    "Trial |validation loss |validation accuracy |training loss |training accuracy\n",
    "-|-|-|-|-\n",
    "1|4.1155|0.1032|2.7049|0.2644\n",
    "2|4.8277|0.0986|2.6190|0.3976\n",
    "\n",
    "With Every Pixel Gaussian Noise: (2 epochs used)\n",
    "\n",
    "Trial |validation loss |validation accuracy |training loss |training accuracy\n",
    "-|-|-|-|-\n",
    "\n",
    "Normal 50,000 training:\n",
    "\n",
    "Architecture:\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "losses: 2.35, 2.4 (training, val); \n",
    "accuracy: 0.146, 0.1139\n",
    "\n",
    "Similar to previous architecture's results \n",
    "\n",
    "This is likely significant (i.e. our net does stuff with lots of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f49dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fbede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

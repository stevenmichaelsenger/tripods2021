{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ce0cc0",
   "metadata": {},
   "source": [
    "# Using Noise by Perturbing Each Image in a Certain Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a05f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import random\n",
    "\n",
    "# returns the CIFAR-10 data set (info here: https://keras.io/api/datasets/cifar10/)\n",
    "# The data set loaded is has 50,000 training images and 10,000 testing images, but the function below only takes\n",
    "# a certain number train_size of training images from the original training set. The rest of the training examples\n",
    "# are added to the test images.\n",
    "def generate_data(train_size):\n",
    "    # loads the data in 50000,10000 form with labels (y) as integers, not one-hot\n",
    "    (x_train_bef, y_train_bef), (x_test_bef, y_test_bef) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    # shuffles 50000 training data\n",
    "    np.random.shuffle(x_train_bef)\n",
    "    np.random.shuffle(y_train_bef)\n",
    "    \n",
    "    # cuts off train_size amounts of sample data\n",
    "    x_train = x_train_bef[:train_size]\n",
    "    y_train = y_train_bef[:train_size]\n",
    "    \n",
    "    # adds the rest of the training data to the test data\n",
    "    x_test = np.concatenate((x_train_bef[train_size:], x_test_bef), axis=0)\n",
    "    y_test = np.concatenate((y_train_bef[train_size:], y_test_bef), axis=0)\n",
    "    \n",
    "    # turns image arrays into floats just so that everything is a float, not an int\n",
    "    x_train = x_train.astype(np.float)\n",
    "    x_test = x_test.astype(np.float)\n",
    "    \n",
    "    # turns label data into one-hot form\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# looks at the training data, clones it, but then creates small changes to each image\n",
    "# Currently, it takes a random 5x5 section of each image and perturbs each section\n",
    "# the input it original training data which is not changed, but copied and then perturbed: the perturbed result\n",
    "# is returned\n",
    "def create_perturbed_clone(x_train, y_train):\n",
    "    # new objects that are copied: changing x_train_pert won't change x_train\n",
    "    x_train_pert = x_train.copy()\n",
    "    y_train_pert = y_train.copy()\n",
    "    \n",
    "    # for each image in x_train_pert we look at a random location and perturb the 5x5 region\n",
    "    length=x_train_pert.shape[0]\n",
    "    for i in range(length):\n",
    "        for repeat in range(3):\n",
    "            loc_x = random.randrange(2,30) # x-coord of center of 5x5 region\n",
    "            loc_y = random.randrange(2,30) # y-coord of center of 5x5 region\n",
    "            \n",
    "            # each pixel and each RGB value is perturbed\n",
    "            for u in [-2,-1,0,1,2]:\n",
    "                for v in [-2,-1,0,1,2]:\n",
    "                    x_train_pert[i][loc_x+u][loc_y+v][0] += 50*(random.random()-0.5)\n",
    "                    x_train_pert[i][loc_x+u][loc_y+v][1] += 50*(random.random()-0.5)\n",
    "                    x_train_pert[i][loc_x+u][loc_y+v][2] += 50*(random.random()-0.5)\n",
    "    \n",
    "    return x_train_pert, y_train_pert\n",
    "\n",
    "# takes training data and adds a perturbed copy to make the training set enlarge_factor times larger\n",
    "def create_noise(x_train, y_train, enlarge_factor):\n",
    "    x_train_noisy = x_train.copy()\n",
    "    y_train_noisy = y_train.copy()\n",
    "    for i in range(enlarge_factor-1):\n",
    "        x_add, y_add = create_perturbed_clone(x_train, y_train)\n",
    "        x_train_noisy = np.concatenate((x_train_noisy, x_add))\n",
    "        y_train_noisy = np.concatenate((y_train_noisy, y_add))\n",
    "    return x_train_noisy, y_train_noisy\n",
    "\n",
    "# creates network: we use a convolutional neural network which makes sense for this problem\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    #model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(70, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax')) # output lyer is 10-dimension one-hot, so softmax is used\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run_model(Epochs, BatchSize, trainSize, Set_enlarge, noisy=False):\n",
    "    x_train, y_train, x_test, y_test = generate_data(trainSize)\n",
    "    if noisy:\n",
    "        x_train, y_train = create_noise(x_train, y_train, Set_enlarge)\n",
    "    MODEL = create_model()\n",
    "    MODEL.fit(x=x_train, y=y_train, epochs=Epochs, batch_size=BatchSize)\n",
    "    MODEL.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebe99bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1407/1407 [==============================] - 142s 99ms/step - loss: 5.2289 - accuracy: 0.2574\n",
      "Epoch 2/8\n",
      "1407/1407 [==============================] - 146s 104ms/step - loss: 1.6368 - accuracy: 0.8204\n",
      "Epoch 3/8\n",
      "1407/1407 [==============================] - 147s 105ms/step - loss: 1.3217 - accuracy: 0.9051\n",
      "Epoch 4/8\n",
      "1407/1407 [==============================] - 146s 104ms/step - loss: 1.2387 - accuracy: 0.9192\n",
      "Epoch 5/8\n",
      "1407/1407 [==============================] - 147s 104ms/step - loss: 1.1950 - accuracy: 0.9260\n",
      "Epoch 6/8\n",
      "1407/1407 [==============================] - 149s 106ms/step - loss: 1.1441 - accuracy: 0.9309\n",
      "Epoch 7/8\n",
      "1407/1407 [==============================] - 142s 101ms/step - loss: 1.1195 - accuracy: 0.9349\n",
      "Epoch 8/8\n",
      "1407/1407 [==============================] - 144s 102ms/step - loss: 1.1266 - accuracy: 0.9386\n",
      "1688/1688 [==============================] - 26s 15ms/step - loss: 8.0846 - accuracy: 0.1001\n"
     ]
    }
   ],
   "source": [
    "# Whether with or without noise, the model is accurate about 0.1 of the time on the test data, i.e. it is no\n",
    "# better than a random guesser.\n",
    "run_model(8,64,6000, noisy=True, Set_enlarge=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287a528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
